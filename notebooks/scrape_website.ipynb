{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.therocketbrew.com/blogs/what-data-to-use-to-personalize-cold-outreach\n",
      "https://www.therocketbrew.com/\n",
      "https://www.therocketbrew.com/team\n",
      "https://www.therocketbrew.com/startup\n",
      "https://www.therocketbrew.com/features\n",
      "http://www.therocketbrew.com/sales-and-crm-integrations\n",
      "https://www.therocketbrew.com/checkout\n",
      "http://www.therocketbrew.com/blogs\n",
      "https://www.therocketbrew.com/pricing\n",
      "http://www.therocketbrew.com/blogs/what-are-the-key-elements-of-a-good-value-proposition\n",
      "https://www.therocketbrew.com/blogs\n",
      "https://www.therocketbrew.com/enterprise\n",
      "http://www.therocketbrew.com/security-and-trust\n",
      "https://www.therocketbrew.com/book-a-demo\n",
      "http://www.therocketbrew.com/email-101-how-to-set-up-dns-records-email-auth-for-gmail-using-rocketbrew\n",
      "len(links)=15\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_all_links(url):\n",
    "    try:\n",
    "        # Step 1: Fetch the HTML content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        html_content = response.text\n",
    "\n",
    "        # Step 2: Parse the HTML\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Step 3: Extract the links\n",
    "        links = []\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            link = urljoin(url, a_tag['href'])  # Convert relative URLs to absolute\n",
    "            links.append(link)\n",
    "\n",
    "        clean_links = get_clean_links(links)\n",
    "        same_domain_links = filter_different_domain_links(clean_links, url)\n",
    "        return same_domain_links\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return []\n",
    "    \n",
    "def get_clean_links(links: list) -> list:\n",
    "    links = list(set(links))\n",
    "    links = [link for link in links if link.startswith('https://www.') or link.startswith('http://www.')]\n",
    "\n",
    "    return links\n",
    "\n",
    "def filter_different_domain_links(links: list, url) -> list:\n",
    "    domain = url.replace('https://www.', '').replace('http://www.', '').split('.', maxsplit=1)[0]\n",
    "\n",
    "    links = [link for link in links \n",
    "             if link.replace('https://www.', '').replace('http://www.', '').startswith(domain)]\n",
    "    return links\n",
    "\n",
    "# Usage\n",
    "url = 'https://www.therocketbrew.com/'  \n",
    "links = get_all_links(url)\n",
    "for link in links:\n",
    "    print(link)\n",
    "\n",
    "print(f\"{len(links)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'therocketbrew'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain = url.replace('https://www.', '').replace('http://www.', '').split('.', maxsplit=1)[0]\n",
    "domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide what links are useful to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv();\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "I'm scraping website to learn more about the product it offers, what are the benefits of using it and what are the competitive advantages of this company or its products. \n",
    "\n",
    "Help me decide what 10 links out of all links should I scrape. Return the JSON with key \"useful_links\" and value is list of links.\n",
    "\n",
    "My life depends on this. I will tip you generously if you follow the instructions and do a great job. Only return the JSON with key \"useful_links\" and value is list of links, no other text is needed.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sales",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
